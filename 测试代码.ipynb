{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_graph_completion.data import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB15k-237 数据集加载完成。\n",
      "训练集大小: 272115 条三元组\n",
      "验证集大小: 17535 条三元组\n",
      "测试集大小: 20466 条三元组\n"
     ]
    }
   ],
   "source": [
    "# 加载 FB15k-237 数据集的训练、验证和测试集\n",
    "train_data, valid_data, test_data = data_loader.load_fb15k237(\"./knowledge_graph_completion/data/FB15k-237/\")\n",
    "print(\"FB15k-237 数据集加载完成。\")\n",
    "print(f\"训练集大小: {len(train_data)} 条三元组\")\n",
    "print(f\"验证集大小: {len(valid_data)} 条三元组\")\n",
    "print(f\"测试集大小: {len(test_data)} 条三元组\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_name</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dominican_Republic</td>\n",
       "      <td>/location/country/form_of_government</td>\n",
       "      <td>Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mighty_Morphin_Power_Rangers</td>\n",
       "      <td>/tv/tv_program/regular_cast./tv/regular_tv_app...</td>\n",
       "      <td>Wendee_Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drama</td>\n",
       "      <td>/media_common/netflix_genre/titles</td>\n",
       "      <td>American_History_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michelle_Rodriguez</td>\n",
       "      <td>/award/award_winner/awards_won./award/award_ho...</td>\n",
       "      <td>Naveen_Andrews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia_national_association_football_team</td>\n",
       "      <td>/soccer/football_team/current_roster./sports/s...</td>\n",
       "      <td>Midfielder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      head_name  \\\n",
       "0                            Dominican_Republic   \n",
       "1                  Mighty_Morphin_Power_Rangers   \n",
       "2                                         Drama   \n",
       "3                            Michelle_Rodriguez   \n",
       "4  Australia_national_association_football_team   \n",
       "\n",
       "                                            relation           tail_name  \n",
       "0               /location/country/form_of_government            Republic  \n",
       "1  /tv/tv_program/regular_cast./tv/regular_tv_app...          Wendee_Lee  \n",
       "2                 /media_common/netflix_genre/titles  American_History_X  \n",
       "3  /award/award_winner/awards_won./award/award_ho...      Naveen_Andrews  \n",
       "4  /soccer/football_team/current_roster./sports/s...          Midfielder  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[['head_name','relation','tail_name']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数便于打印三元组列表的前几项\n",
    "def preview_triples(name, df, num=5):\n",
    "    print(f\"\\n{name} 集合样本前 {num} 条：\")\n",
    "    for _, row in df.head(num).iterrows():\n",
    "        print(f\"{row['head_name']}\\t{row['relation']}\\t{row['tail_name']}\")\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "训练 集合样本前 5 条：\n",
      "Dominican_Republic\t/location/country/form_of_government\tRepublic\n",
      "Mighty_Morphin_Power_Rangers\t/tv/tv_program/regular_cast./tv/regular_tv_appearance/actor\tWendee_Lee\n",
      "Drama\t/media_common/netflix_genre/titles\tAmerican_History_X\n",
      "Michelle_Rodriguez\t/award/award_winner/awards_won./award/award_honor/award_winner\tNaveen_Andrews\n",
      "Australia_national_association_football_team\t/soccer/football_team/current_roster./sports/sports_team_roster/position\tMidfielder\n",
      "...\n",
      "\n",
      "验证 集合样本前 5 条：\n",
      "American_Pie\t/film/film/genre\tRomance_Film\n",
      "St._Louis\t/location/location/time_zones\tCentral_Time_Zone\n",
      "George_Burns\t/people/person/spouse_s./people/marriage/type_of_union\tMarriage\n",
      "Primetime_Emmy_Award_for_Outstanding_Writing_-_Drama_Series\t/award/award_category/winners./award/award_honor/award_winner\tDavid_Chase\n",
      "Silent_Hill\t/film/film/release_date_s./film/film_regional_release_date/film_release_region\tLithuania\n",
      "...\n",
      "\n",
      "测试 集合样本前 5 条：\n",
      "Zürich\t/travel/travel_destination/climate./travel/travel_destination_monthly_climate/month\tOctober\n",
      "Autoharp\t/music/performance_role/regular_performances./music/group_membership/group\tHeart\n",
      "Winnie_the_Pooh\t/film/film/release_date_s./film/film_regional_release_date/film_release_region\tFrance\n",
      "England\t/location/location/contains\tPontefract\n",
      "England\t/location/location/contains\tLancaster\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "preview_triples(\"训练\", train_data, num=5)\n",
    "preview_triples(\"验证\", valid_data, num=5)\n",
    "preview_triples(\"测试\", test_data, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 构建 Few-Shot 提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = list(train_data[['head_name','relation','tail_name']].itertuples(index=False, name=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集中选择关系为 \"founded\" 的 few-shot 示例\n",
    "relation = \"/location/country/form_of_government\"\n",
    "K = 3  # 选取3条few-shot示例\n",
    "fewshot_examples = [trip for trip in triples if trip[1] == relation][:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dominican_Republic', '/location/country/form_of_government', 'Republic'),\n",
       " ('Saint_Vincent_and_the_Grenadines',\n",
       "  '/location/country/form_of_government',\n",
       "  'Parliamentary_system'),\n",
       " ('Liechtenstein',\n",
       "  '/location/country/form_of_government',\n",
       "  'Parliamentary_system')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "构建的提示 Prompt:\n",
      "Dominican_Republic - /location/country/form_of_government -> Republic\n",
      "Saint_Vincent_and_the_Grenadines - /location/country/form_of_government -> Parliamentary_system\n",
      "Liechtenstein - /location/country/form_of_government -> Parliamentary_system\n",
      "Dominican_Republic - /location/country/form_of_government ->\n"
     ]
    }
   ],
   "source": [
    "query_head = \"Dominican_Republic\"\n",
    "query_relation = \"/location/country/form_of_government\"\n",
    "prompt = \"\"\n",
    "for h, r, t in fewshot_examples:\n",
    "    prompt += f\"{h} - {r} -> {t}\\n\"\n",
    "prompt += f\"{query_head} - {query_relation} ->\"\n",
    "\n",
    "print(\"\\n构建的提示 Prompt:\\n\" + prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型推理生成尾实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 Qwen3-0.6B 模型和分词器\n",
    "model_name = \"./knowledge_graph_completion/models/Qwen3-0.6B/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)                      # 把模型权重移动到 GPU\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将提示编码为模型输入并生成文本\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to(device)            # GPU Tensor\n",
    "attention_mask = inputs.attention_mask.to(device)  # 如果有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/kgc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/kgc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/kgc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=10,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    只取 prompt 后生成部分\n",
    "generated_ids = outputs[0, input_ids.shape[1]:].cpu()\n",
    "raw = tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测尾实体： Parliamentary_system\n"
     ]
    }
   ],
   "source": [
    "# 只保留第一行（遇到换行就截断）\n",
    "pred = raw.splitlines()[0].strip()\n",
    "print(\"预测尾实体：\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hits@1 和 MRR 指标评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 评估前 N 条测试集样本\n",
    "N = min(1000, len(test_data))\n",
    "hits1_count = 0\n",
    "ranks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [06:23<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (_, row) in enumerate(tqdm(test_data.head(N).iterrows(), total=N, desc=\"Evaluating\")):\n",
    "    \n",
    "    # 直接从 row 里取列：\n",
    "    h = row['head_name']      # 或者 row['head']，如果想用 MID\n",
    "    r = row['relation']       # 原始谓词标识\n",
    "    true_t = row['tail_name'] # 或者 row['tail']\n",
    "    \n",
    "    # 构建 Few-Shot 提示\n",
    "    fs = train_data[train_data['relation'] == r].head(3)\n",
    "    prompt_i = \"\"\n",
    "    for _, ex in fs.iterrows():\n",
    "        prompt_i += f\"{ex['head_name']} - {ex['relation']} -> {ex['tail_name']}\\n\"\n",
    "    prompt_i += f\"{h} - {r} ->\"\n",
    "    \n",
    "    # 模型生成预测\n",
    "    inputs_i = tokenizer(prompt_i, return_tensors=\"pt\")\n",
    "    input_ids = inputs_i.input_ids.to(device)            # GPU Tensor\n",
    "    attention_mask = inputs_i.attention_mask.to(device)  # 如果有\n",
    "    outputs = model.generate(input_ids=input_ids,attention_mask=attention_mask,max_new_tokens=10,do_sample=False)\n",
    "    generated_ids = outputs[0, input_ids.shape[1]:].cpu()\n",
    "    pred = tokenizer.decode(generated_ids, skip_special_tokens=True).splitlines()[0].strip()\n",
    "\n",
    "    # 统计 Hits@1 与 MRR\n",
    "    hit = int(pred == true_t)\n",
    "    hits1_count += hit\n",
    "    ranks.append(1.0 if hit else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1 = 0.1470, MRR = 0.1470\n"
     ]
    }
   ],
   "source": [
    "hits1 = hits1_count / N\n",
    "mrr   = sum(ranks) / N\n",
    "print(f\"Hits@1 = {hits1:.4f}, MRR = {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/kgc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from knowledge_graph_completion.infer import evaluate_model\n",
    "from knowledge_graph_completion.utils import calculate_metrics\n",
    "from knowledge_graph_completion.data.data_loader import load_fb15k237\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据（已含 head_name, tail_name, relation_name）\n",
    "train_data, valid_data, test_data = load_fb15k237(\"./knowledge_graph_completion/data/FB15k-237/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>head_name</th>\n",
       "      <th>tail_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/m/027rn</td>\n",
       "      <td>/location/country/form_of_government</td>\n",
       "      <td>/m/06cx9</td>\n",
       "      <td>Dominican_Republic</td>\n",
       "      <td>Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/m/017dcd</td>\n",
       "      <td>/tv/tv_program/regular_cast./tv/regular_tv_app...</td>\n",
       "      <td>/m/06v8s0</td>\n",
       "      <td>Mighty_Morphin_Power_Rangers</td>\n",
       "      <td>Wendee_Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/m/07s9rl0</td>\n",
       "      <td>/media_common/netflix_genre/titles</td>\n",
       "      <td>/m/0170z3</td>\n",
       "      <td>Drama</td>\n",
       "      <td>American_History_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/01sl1q</td>\n",
       "      <td>/award/award_winner/awards_won./award/award_ho...</td>\n",
       "      <td>/m/044mz_</td>\n",
       "      <td>Michelle_Rodriguez</td>\n",
       "      <td>Naveen_Andrews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/m/0cnk2q</td>\n",
       "      <td>/soccer/football_team/current_roster./sports/s...</td>\n",
       "      <td>/m/02nzb8</td>\n",
       "      <td>Australia_national_association_football_team</td>\n",
       "      <td>Midfielder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         head                                           relation       tail  \\\n",
       "0    /m/027rn               /location/country/form_of_government   /m/06cx9   \n",
       "1   /m/017dcd  /tv/tv_program/regular_cast./tv/regular_tv_app...  /m/06v8s0   \n",
       "2  /m/07s9rl0                 /media_common/netflix_genre/titles  /m/0170z3   \n",
       "3   /m/01sl1q  /award/award_winner/awards_won./award/award_ho...  /m/044mz_   \n",
       "4   /m/0cnk2q  /soccer/football_team/current_roster./sports/s...  /m/02nzb8   \n",
       "\n",
       "                                      head_name           tail_name  \n",
       "0                            Dominican_Republic            Republic  \n",
       "1                  Mighty_Morphin_Power_Rangers          Wendee_Lee  \n",
       "2                                         Drama  American_History_X  \n",
       "3                            Michelle_Rodriguez      Naveen_Andrews  \n",
       "4  Australia_national_association_football_team          Midfielder  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载分词器与模型\n",
    "model_name = \"./knowledge_graph_completion/models/Qwen3-0.6B/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batched Evaluating: 100%|██████████| 63/63 [04:49<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "N = min(1000, len(test_data))\n",
    "hits1, mrr = evaluate_model(model, tokenizer, test_data.head(N), train_data, num_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1 = 0.1510, MRR = 0.1628\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hits@1 = {hits1:.4f}, MRR = {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_graph_completion.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建微调样本（随机挑 1000 条）\n",
    "sample_df_train = train_data.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "sample_df_test = test_data.sample(n=1000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 625/625 [24:51<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] 平均 Loss: 2.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [06:45<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[验证] Hits@1: 0.0060, MRR: 0.0202\n",
      "→ 保存最佳模型，MRR: 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  11%|█         | 69/625 [02:49<22:42,  2.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# train_model 内部会将验证集上的最佳模型保存在 outputs/best_model/\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_df_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# DataFrame，含 head/relation/tail 和 head_name/... 列\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_df_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# DataFrame，用于早停 & 模型选择\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./outputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# best_model 将保存在 outputs/best_model/\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m微调训练完毕，最佳模型已保存到 outputs/best_model/\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/szl/KG/knowledge_graph_completion/train.py:96\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, tokenizer, train_data, valid_data, epochs, batch_size, learning_rate, output_dir, num_examples, top_k)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# 平均并反向\u001b[39;00m\n\u001b[32m     95\u001b[39m batch_loss = batch_loss / valid_count\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mbatch_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m optimizer.step()\n\u001b[32m     98\u001b[39m batch_loss_item = batch_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kgc/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    513\u001b[39m         Tensor.backward,\n\u001b[32m    514\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m         inputs=inputs,\n\u001b[32m    520\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kgc/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    284\u001b[39m     retain_graph = create_graph\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/kgc/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    767\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train_model 内部会将验证集上的最佳模型保存在 outputs/best_model/\n",
    "train_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_data=sample_df_train,      # DataFrame，含 head/relation/tail 和 head_name/... 列\n",
    "    valid_data=sample_df_test,      # DataFrame，用于早停 & 模型选择\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    output_dir=\"./outputs\",     # best_model 将保存在 outputs/best_model/\n",
    "    num_examples=3,\n",
    "    top_k=10\n",
    ")\n",
    "print(\"微调训练完毕，最佳模型已保存到 outputs/best_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dir = \"outputs/best_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_dir)\n",
    "model     = AutoModelForCausalLM.from_pretrained(best_dir).to(device)\n",
    "model.eval()\n",
    "print(\"已加载本地最佳模型。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits1, mrr = evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_data,   # 这里只评估前1000条以加快速度\n",
    "    train_data,             # Few-Shot 示例来源\n",
    "    num_examples=3,\n",
    "    top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# 取训练集里随机 1000 条三元组做示例微调\n",
    "sample_df = train_data.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for _, row in sample_df.iterrows():\n",
    "        h = row['head_name']\n",
    "        r = row['relation_name']\n",
    "        t = row['tail_name']\n",
    "        \n",
    "        # 构建硬提示：只用当前三元组，不插 Few‑Shot 示例\n",
    "        prompt_train = f\"{h} - {r} -> {t}\"\n",
    "        # 让模型学习在提示后续生成尾实体 t\n",
    "        inputs = tokenizer(prompt_train, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        # 标签与输入一致，让模型自回归学习\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(sample_df)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} — 平均 Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 切回评估模式\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
